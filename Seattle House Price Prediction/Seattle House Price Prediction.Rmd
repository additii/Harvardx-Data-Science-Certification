---
title: "Seattle House Price Prediction"
author: "Aditi Gupta"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  - |
    ```{=latex}
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
    ```
---

# 1. Introduction

This project analyzes a dataset of Seattle housing prices, sourced from Kaggle, to develop predictive models for home prices based on various property attributes. The dataset contains essential variables, including the number of beds and baths, lot size, and living area size. Our primary objective was to predict house prices using regression models, comparing the accuracy of multiple approaches. Key project steps involved data cleaning, feature transformation, exploratory data analysis (EDA), and model building with various algorithms to identify the best predictive model.


# 2. Methods and Analysis

## 2.1 Data Cleaning and Preprocessing:

### Summary:
Unit Conversion: Lot size units were converted to square feet wherever necessary, specifically converting acres to square feet.
Handling Missing Values: Missing values in the lot_size column were replaced with values from the size column where applicable. Missing entries in the lot_size_units column were filled based on the size_units.
Log Transformation: To normalize skewed data distributions, log transformations were applied to price, size, and lot_size columns.
        
```{r load_preprocess_data,echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}

# Loading necessary libraries
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")


library(readr)
library(caret)
library(randomForest)
library(xgboost)
library(caTools)
library(dplyr)
library(ggplot2)
library(corrplot)
library(Metrics)
library(e1071)
library(gbm)

# Setting seed for reproducibility
set.seed(123)

# Step 1: Loading Training and Testing Data Separately
path_to_file <- "C:\\Users\\Aditi\\OneDrive\\Documents\\Harvardx-Data-Science-Certification\\Housing price prediction seattle\\house_price_data.csv"
data <- read_csv(path_to_file)

# Inspecting structure and summary
str(data)
summary(data)

# Step 2: Data Cleaning and Pre-processing

# Converting lot_size_units to Square Feet (sqft)
unique(data$lot_size_units)

# Converting non-sqft units to sqft
data <- data %>%
  mutate(lot_size = ifelse(lot_size_units == "acres", lot_size * 43560, lot_size),
         lot_size_units = "sqft")

# Handling Missing Values
data$lot_size[is.na(data$lot_size)] <- data$size[is.na(data$lot_size)]
data$lot_size_units[is.na(data$lot_size_units)] <- data$size_units[is.na(data$lot_size_units)]

# Ensuring consistency in Numerical Columns
data$beds <- as.numeric(data$beds)
data$baths <- as.numeric(data$baths)
data$size <- as.numeric(data$size)
data$lot_size <- as.numeric(data$lot_size)
data$price <- as.numeric(data$price)

# Applying log transformation where applicable
data$log_price <- log(data$price)
data$log_size <- log(data$size)
data$log_lot_size <- log(data$lot_size)

# Removing 'size_units' and 'lot_size_units' from the dataset for modeling
data <- data %>% select(-size_units, -lot_size_units, -price, -size, -lot_size)

```

## 2.2 Exploratory Data Analysis:
        
        
### 2.2.1 Correlation Analysis: 
A correlation matrix of key variables (beds, baths, log_size, log_lot_size, log_price) was visualized, highlighting relationships and dependencies.
        
#### Insights:

>log_price shows a high positive correlation with log_size (0.77), indicating that the size of the house is a strong predictor of price.
>beds and baths are also positively correlated with log_price but to a lesser extent, suggesting they contribute to house price predictions, though less significantly than size.
>log_lot_size has a weaker correlation with log_price, indicating that the land size may influence price but is less critical compared to other features.
    
```{r corr,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Correlation matrix plot (using log-transformed variables)
library(corrplot)

# Calculate correlation matrix
cor_matrix <- cor(data %>% select(beds, baths, log_size, log_lot_size, log_price), use = "complete.obs")

# Plot correlation matrix with correlation values displayed
corrplot(
  cor_matrix,
  method = "circle",
  type = "upper",
  addCoef.col = "black",
  tl.col = "black",
  tl.srt = 45,
  number.cex = 0.7,
  col = colorRampPalette(c("blue", "white", "red"))(200)
)
```

### 2.2.2 Distribution Analysis: 
Histograms were plotted for log-transformed price, size, and lot size, helping to identify data spread and central tendencies. Scatter plots further illustrated relationships between price and other features such as the number of beds, baths, and log-transformed size variables.

To understand the spread and central tendency, histograms for log-transformed price, size, and lot_size were generated:

#### Log Price Distribution:
The histogram for log_price shows a normal distribution with a mean price around 15 (in log scale), suggesting a general trend in house prices within this range.
Properties with prices above this mean likely include premium or larger homes.

```{r pricedist,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Distribution Analysis with mean lines
# Calculate mean values
mean_log_price <- mean(data$log_price, na.rm = TRUE)
mean_log_size <- mean(data$log_size, na.rm = TRUE)
mean_log_lot_size <- mean(data$log_lot_size, na.rm = TRUE)

# Log Price distribution with mean line
ggplot(data, aes(x = log_price)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_vline(xintercept = mean_log_price, color = "red", linetype = "dashed", size = 1) +
  theme_minimal() +
  labs(title = "Distribution of Log House Prices", x = "Log Price", y = "Frequency")
```

#### Log Size Distribution:
The histogram for log_size indicates most properties have a size around 7-8 (log scale), which aligns with typical residential sizes in Seattle.
Homes with a higher log size are relatively rare and could represent luxury properties or those with significant extensions.

```{r sizedist,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Log Size distribution with mean line
ggplot(data, aes(x = log_size)) +
  geom_histogram(bins = 30, fill = "lightgreen", color = "black") +
  geom_vline(xintercept = mean_log_size, color = "red", linetype = "dashed", size = 1) +
  theme_minimal() +
  labs(title = "Distribution of Log House Size", x = "Log Size (sqft)", y = "Frequency")
```

####  Log Lot Size Distribution:
The distribution of log_lot_size reveals a right-skewed pattern, indicating that smaller lot sizes are more common in the dataset.
A smaller number of homes have significantly larger lot sizes, which may reflect properties in suburban or rural areas.
        
```{r lotsize,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Log Lot size distribution with mean line
ggplot(data, aes(x = log_lot_size)) +
  geom_histogram(bins = 30, fill = "lightcoral", color = "black") +
  geom_vline(xintercept = mean_log_lot_size, color = "red", linetype = "dashed", size = 1) +
  theme_minimal() +
  labs(title = "Distribution of Log Lot Size", x = "Log Lot Size (sqft)", y = "Frequency")
```

### 2.2.3 Relationship with Log Price:

Scatter plots were used to analyze relationships between log_price and other key features:

####  Log Price vs. Beds:
There is a positive relationship between log_price and beds, with price generally increasing as the number of bedrooms grows. 
Outliers indicate some homes with fewer bedrooms still command high prices, likely due to other premium features.

```{r price_beds,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Log Price vs Beds
ggplot(data, aes(x = beds, y = log_price)) +
  geom_point(color = "blue") +
  theme_minimal() +
  labs(title = "Log Price vs Beds", x = "Number of Beds", y = "Log Price")
```

#### Log Price vs. Baths:
A similar trend exists between log_price and baths, where more bathrooms correlate with higher prices.
High-priced homes with fewer bathrooms may have other luxury features contributing to their value.


```{r price_baths,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}


# Log Price vs Baths
ggplot(data, aes(x = baths, y = log_price)) +
  geom_point(color = "purple") +
  theme_minimal() +
  labs(title = "Log Price vs Baths", x = "Number of Baths", y = "Log Price")
```

####  Log Price vs. Log Size:
A strong positive relationship is observed, reinforcing that larger homes tend to be more expensive.
The tight clustering around the trend line indicates that size is a key determinant of price.

```{r price_size,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Log Price vs Log Size
ggplot(data, aes(x = log_size, y = log_price)) +
  geom_point(color = "darkgreen") +
  theme_minimal() +
  labs(title = "Log Price vs Log Size", x = "Log Size (sqft)", y = "Log Price")
```

  
####  Log Price vs. Log Lot Size:
A weaker positive correlation between log_price and log_lot_size suggests that lot size contributes to price but is less influential than living area size.
Higher-priced properties with smaller lot sizes are likely situated in premium urban areas where land is more valuable.


```{r relation_lot,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Log Price vs Log Lot Size
ggplot(data, aes(x = log_lot_size, y = log_price)) +
  geom_point(color = "orange") +
  theme_minimal() +
  labs(title = "Log Price vs Log Lot Size", x = "Log Lot Size (sqft)", y = "Log Price")

```

# 3 Modeling Approach:
The dataset was divided into training (80%) and testing (20%) sets for validation.

Four predictive models were developed:
## 3.1 Linear Regression: 
A baseline model to understand the general linear relationships in the data.
            
```{r linearR,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Step 4: Data Splitting
set.seed(123)
trainIndex <- createDataPartition(data$log_price, p = .8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Model Training and Tuning
# 1. Linear Regression
lm_model <- train(log_price ~ ., data = trainData, method = "lm")
```

## 3.2 Support Vector Regression (SVR): 
Explored as a more complex model that can handle non-linearity.
            
```{r svr,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# 2. Support Vector Regression (SVR)
train_control <- trainControl(method = "cv", number = 5)
svr_model <- train(
  log_price ~ ., data = trainData, method = "svmRadial",
  trControl = train_control, preProcess = c("center", "scale"), tuneLength = 5
)
```

## 3.3 Gradient Boosting Machine (GBM): 
Implemented as an advanced ensemble learning model for enhanced predictive performance.
            
```{r gbm,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# 3. Gradient Boosting
gbm_grid <- expand.grid(interaction.depth = c(3, 5, 7), n.trees = 100, shrinkage = 0.1, n.minobsinnode = 10)
gbm_model <- train(log_price ~ ., data = trainData, method = "gbm", trControl = train_control, tuneGrid = gbm_grid, verbose = FALSE)

```

## 3.4 XGBoost: 
Built as an optimized gradient-boosting model to exploit potential non-linear relationships, with hyperparameter tuning to maximize model accuracy.
            
```{r xgboost,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# 4. XGBoost
xgb_grid <- expand.grid(nrounds = 100, max_depth = 6, eta = 0.1, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1, subsample = 0.8)
xgb_model <- train(log_price ~ ., data           = trainData, method = "xgbTree", trControl = train_control, tuneGrid = xgb_grid)

```


## 3.5 Hyperparameter Tuning for XGBoost:
An extensive grid search optimized parameters like max_depth, eta, gamma, colsample_bytree, min_child_weight, and subsample through cross-validation to minimize RMSE and improve accuracy.
        
```{r tunning_xgboost,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Step 5: Optimized Hyperparameter Tuning for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(trainData %>% select(-log_price)), label = trainData$log_price)
dtest <- xgb.DMatrix(data = as.matrix(testData %>% select(-log_price)), label = testData$log_price)

param_grid <- list(
  max_depth = c(3, 5, 7),
  eta = c(0.05, 0.1, 0.2),
  gamma = c(0, 0.1, 0.2),
  colsample_bytree = c(0.7, 0.8, 0.9),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.7, 0.8, 0.9)
)

best_rmse <- Inf
best_params <- list()

for (max_depth in param_grid$max_depth) {
  for (eta in param_grid$eta) {
    for (gamma in param_grid$gamma) {
      for (colsample_bytree in param_grid$colsample_bytree) {
        for (min_child_weight in param_grid$min_child_weight) {
          for (subsample in param_grid$subsample) {
            params <- list(
              booster = "gbtree",
              objective = "reg:squarederror",
              max_depth = max_depth,
              eta = eta,
              gamma = gamma,
              colsample_bytree = colsample_bytree,
              min_child_weight = min_child_weight,
              subsample = subsample
            )
            cv <- xgb.cv(
              params = params,
              data = dtrain,
              nrounds = 150,
              nfold = 5,
              metrics = "rmse",
              early_stopping_rounds = 10,
              verbose = 0
            )
            mean_rmse <- min(cv$evaluation_log$test_rmse_mean)
            best_iteration <- cv$best_iteration
            if (mean_rmse < best_rmse) {
              best_rmse <- mean_rmse
              best_params <- params
              best_params$nrounds <- best_iteration
            }
          }
        }
      }
    }
  }
}

final_model <- xgb.train(params = best_params, data = dtrain, nrounds = best_params$nrounds)
tuned_test_predictions <- predict(final_model, dtest, iterationrange = c(1, best_params$nrounds))

```


# 4. Results

## 4.1 Model Evaluation: 
Each model was evaluated on the test set using metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). The results are summarized below:
    
```{r evaluation,echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Step 6: Model Evaluation and Comparison
evaluate_model <- function(model, data) {
  predictions <- predict(model, data)
  mse_val <- mse(data$log_price, predictions)
  mae_val <- mae(data$log_price, predictions)
  rmse_val <- rmse(data$log_price, predictions)
  return(data.frame(MSE = mse_val, MAE = mae_val, RMSE = rmse_val))
}

lm_results <- evaluate_model(lm_model, testData)
svr_results <- evaluate_model(svr_model, testData)
gbm_results <- evaluate_model(gbm_model, testData)
xgb_results <- evaluate_model(xgb_model, testData)
final_xgb_results <- data.frame(MSE = mse(testData$log_price, tuned_test_predictions),
                                MAE = mae(testData$log_price, tuned_test_predictions),
                                RMSE = rmse(testData$log_price, tuned_test_predictions))

results <- rbind(lm_results, svr_results, gbm_results, xgb_results, final_xgb_results)
row.names(results) <- c("Linear Regression", "Support Vector Regression", "Gradient Boosting", "XGBoost", "Tuned XGBoost")
print("Model Evaluation Results:")
print(results)

best_model_name <- row.names(results)[which.min(results$RMSE)]
best_model_rmse <- min(results$RMSE)
print(paste("Best Model:", best_model_name, "with RMSE:", round(best_model_rmse, 4)))

# Set best model as `final_model`
if (best_model_name == "Linear Regression") {
  best_model_predictions <- predict(lm_model, testData)
} else if (best_model_name == "Support Vector Regression") {
  best_model_predictions <- predict(svr_model, testData)
} else if (best_model_name == "Gradient Boosting") {
  best_model_predictions <- predict(gbm_model, testData)
} else if (best_model_name == "XGBoost") {  
  best_model_predictions <- predict(xgb_model, testData)
} else {
  best_model_predictions <- tuned_test_predictions
}

# Final evaluation on test data with the best model
final_mse <- mse(testData$log_price, best_model_predictions)
final_mae <- mae(testData$log_price, best_model_predictions)
final_rmse <- rmse(testData$log_price, best_model_predictions)
final_results <- data.frame(MSE = final_mse, MAE = final_mae, RMSE = final_rmse)
print("Final Model Performance on Test Data:")
print(final_results)
```

Best Model: The XGBoost model achieved the lowest RMSE (0.249), demonstrating the highest predictive accuracy among all models tested.

Final Model Performance: The tuned XGBoost model was further validated, achieving an RMSE of 0.249, indicating strong predictive power for estimating house prices based on the dataset features.

# 5. Conclusion

This project successfully developed a predictive model for Seattle house prices, revealing that advanced machine learning techniques, particularly XGBoost, can provide greater accuracy over simpler linear models. The tuned XGBoost model demonstrated the best performance, achieving an RMSE below 0.25 on log-transformed prices. Limitations of this study include potential data biases and limited generalizability beyond Seattle. Future work could incorporate more contextual features (e.g., neighborhood characteristics) and explore advanced techniques such as neural networks for improved predictions.

# 6. References

    
Dataset: Kaggle. (2024). Seattle House Price Prediction Dataset. Retrieved from Kaggle.

https://www.kaggle.com/datasets/samuelcortinhas/house-price-prediction-seattle/data

Libraries and tools: readr, caret, randomForest, xgboost, caTools, dplyr, ggplot2,
    corrplot, Metrics, e1071, gbm in R for data processing, visualization, and modeling.